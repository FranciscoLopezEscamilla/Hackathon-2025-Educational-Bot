{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Agent with supervisor\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "def create_agent(llm: AzureChatOpenAI, tools: list, system_prompt:str):\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            {\n",
    "                \"system\",\n",
    "                system_prompt\n",
    "            },\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools= tools)\n",
    "\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, List, Tuple, Union\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "#tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia tool\n",
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def wikipedia_search_tool(query: str, sentences: int = 5):\n",
    "    \"\"\"Searchs content in the wikipedia web site\"\"\"\n",
    "    \n",
    "    return wikipedia.summary(query, sentences = sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever for RAG\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "api_base = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "embeddings_model = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "                azure_endpoint=api_base,\n",
    "                api_key=api_key,\n",
    "                azure_deployment=embeddings_model)\n",
    "\n",
    "index_path = \"../api/index/faiss_index\"\n",
    "\n",
    "    \n",
    "vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "    next: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def RAG(state: AgentState):\n",
    "    \"\"\"Use this to execute RAG. If the questions is related to gen ai on art or music, using this tool to retrieve resuls\"\"\"\n",
    "\n",
    "    print(\"Calling RAG...\")\n",
    "\n",
    "    question = state\n",
    "    print(\"question: \", question)\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context: {context}\n",
    "    \n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name = name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervisor chain creation\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"RAG\", \"Researcher\", \"Coder\"]\n",
    "\n",
    "system_prompt = \"\"\"You are a supervisor tasked with managing a conversation between the following workers: {members}.\n",
    "\n",
    "Given the following user requests, respond with the worker to act next.\n",
    "\n",
    "Use RAG tool when questions are related to GenAI in Art and GenAI in Music category.\n",
    "\n",
    "Each worker will perform a task and respond with their results and status. \n",
    "\n",
    "when finished, respond with FINISH.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\":{\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\":{\n",
    "            \"next\":{\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options}\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? select one of: {options}\",\n",
    "        ),\n",
    "    ] \n",
    ").partial(options= str(options), members=\", \".join(members))\n",
    "\n",
    "supervisor_chain = (prompt \n",
    "                    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "                    | JsonOutputFunctionsParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of workflow\n",
    "# research node, code node, rag node\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "llm_with_tools = llm.bind_tools([wikipedia_search_tool])\n",
    "\n",
    "research_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but don't know current events\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "research_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | research_prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "research_agent_executor = AgentExecutor(agent=research_agent, tools=[wikipedia_search_tool], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(research_agent_executor.stream({\"input\": [\"Concacaf nations league\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'set'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m      3\u001b[0m research_node \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(agent_node, agent \u001b[38;5;241m=\u001b[39m research_agent_executor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResearcher\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m code_agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython_repl_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou may generate safe python code to analyze data and generate charts using matplotlib.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mcreate_agent\u001b[1;34m(llm, tools, system_prompt)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_agent\u001b[39m(llm: AzureChatOpenAI, tools: \u001b[38;5;28mlist\u001b[39m, system_prompt:\u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43msystem_prompt\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMessagesPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMessagesPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magent_scratchpad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     agent \u001b[38;5;241m=\u001b[39m create_openai_tools_agent(llm, tools, prompt)\n\u001b[0;32m     20\u001b[0m     executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39m tools)\n",
      "File \u001b[1;32mc:\\Users\\p.a.rodriguez.canedo\\Documents\\Hackaton_2025\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1205\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[1;34m(cls, messages, template_format)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   1168\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   1169\u001b[0m     template_format: PromptTemplateFormat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m   1172\u001b[0m \n\u001b[0;32m   1173\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m        a chat prompt template.\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\p.a.rodriguez.canedo\\Documents\\Hackaton_2025\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1002\u001b[0m, in \u001b[0;36mChatPromptTemplate.__init__\u001b[1;34m(self, messages, template_format, **kwargs)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    951\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \n\u001b[0;32m    958\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1002\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\p.a.rodriguez.canedo\\Documents\\Hackaton_2025\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1003\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    951\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \n\u001b[0;32m    958\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1003\u001b[0m         \u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m   1004\u001b[0m     ]\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\p.a.rodriguez.canedo\\Documents\\Hackaton_2025\\.venv\\Lib\\site-packages\\langchain_core\\prompts\\chat.py:1490\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[1;34m(message, template_format)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1489\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _message\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Unsupported message type: <class 'set'>"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "research_node = functools.partial(agent_node, agent = research_agent_executor, name=\"Researcher\")\n",
    "\n",
    "#code_agent = create_agent(llm, \n",
    "#                          [python_repl_tool],\n",
    "#                          \"You may generate safe python code to analyze data and generate charts using matplotlib.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
